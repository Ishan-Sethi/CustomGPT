[
    {
        "content": "describe the purpose of this paper",
        "role": "user",
        "createdAt": "2024-05-08T21:38:25.300Z",
        "id": "2vblbxy"
    },
    {
        "id": "XSQwKVn",
        "role": "assistant",
        "content": "The purpose of this paper is to introduce a new approach to neural network-based machine translation called the Transformer. The authors aim to demonstrate that it is possible to achieve state-of-the-art results in machine translation without relying on recurrent or convolutional neural networks (RNNs/ CNNs), which are commonly used in sequence-to-sequence tasks.\n\nThe Transformer model is designed to directly process input sequences of tokens as a whole, rather than sequentially. This is achieved through self-attention mechanisms that allow the model to attend to different parts of the input sequence simultaneously and weigh their importance. The authors argue that this approach can be more efficient and effective than traditional RNN/CNN-based models.\n\nThe paper also explores the benefits of using self-attention in machine translation, including:\n\n1. Parallelization: Since the Transformer processes input sequences as a whole, it can perform parallel computation across all positions in the sequence.\n2. Reduced sequential dependencies: The self-attention mechanism allows the model to capture long-range dependencies without relying on recurrent or convolutional layers.\n\nThe authors demonstrate the effectiveness of the Transformer model by achieving state-of-the-art results on several machine translation benchmarks, including WMT 2014 and IWSLT 2016.",
        "createdAt": "2024-05-08T21:38:39.399Z",
        "annotations": [
            {
                "type": "events",
                "data": {
                    "title": "Retrieving context for query: 'describe the purpose of this paper'"
                }
            },
            {
                "type": "events",
                "data": {
                    "title": "Retrieved 3 sources to use as context for the query"
                }
            },
            {
                "type": "sources",
                "data": {
                    "nodes": [
                        {
                            "id_": "f9b5eeff-0c52-4fe1-933e-d696ed6eb655",
                            "metadata": {
                                "file_path": "./data/attention.pdf"
                            },
                            "excludedEmbedMetadataKeys": [],
                            "excludedLlmMetadataKeys": [],
                            "relationships": {
                                "SOURCE": {
                                    "nodeId": "5115a8e1-da8f-40cb-80fd-5a6c07fa4443",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "Xe9oUa6hljMJA4FitZvOJsOM9IRi6HlCYSnps2A8qa8="
                                },
                                "PREVIOUS": {
                                    "nodeId": "f9e42d48-fbf3-48cd-a817-d25f7c6e8d99",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "WTWTJQSdJFd29v3STVO+d3oHHwDaKpSRo8+KUpTn6Xc="
                                },
                                "NEXT": {
                                    "nodeId": "2c36b7cb-f9e7-4475-9c40-d82417ab3c7d",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "2z8C0hRSv67EmT/IwjdoZAlyCph3ZDBTrd58qFrdvuI="
                                }
                            },
                            "hash": "gHXOq2ciISVCdOugrzFIWXPqfH+rrrWKzEhJ4BRqq40=",
                            "text": "## Background\n\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. ## Model Architecture\n\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.",
                            "textTemplate": "",
                            "metadataSeparator": "\n",
                            "type": "TEXT",
                            "id": "f9b5eeff-0c52-4fe1-933e-d696ed6eb655",
                            "score": 0.4112419289042685
                        },
                        {
                            "id_": "55b4e300-48bd-4c5b-891f-32dbdaa847eb",
                            "metadata": {
                                "file_path": "./data/attention.pdf"
                            },
                            "excludedEmbedMetadataKeys": [],
                            "excludedLlmMetadataKeys": [],
                            "relationships": {
                                "SOURCE": {
                                    "nodeId": "5115a8e1-da8f-40cb-80fd-5a6c07fa4443",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "Xe9oUa6hljMJA4FitZvOJsOM9IRi6HlCYSnps2A8qa8="
                                },
                                "PREVIOUS": {
                                    "nodeId": "2b706413-5804-4199-900c-36abf8c4552d",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "OFgP+6XOe36Z8RjlvydiL+dBB2v5+n2Hh2wr2rc9zNk="
                                },
                                "NEXT": {
                                    "nodeId": "1d22b264-7111-4423-b1e3-a9ff729fffe9",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "UdCYCZprLOnZkbwP8gsfM3HUD0JIIRxkDuy9g9WRqf4="
                                }
                            },
                            "hash": "K5azG2vtM4tRqnlBQ5h8BYLECIYUZK+gGDFrKRLf7eA=",
                            "text": "|N|dmodel|dff|h|dk|dv|P drop|ϵ ls|train steps|PPL (dev)|BLEU (dev)|params ×10^6|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|base|6|512|2048| |8|64|64|0.1|0.1|100K|4.92|25.8|65|\n| | | |1| | |512|512| | |5.29|24.9|\n| | | |4| | |128|128| | |5.00|25.5|\n| | | |16| | |32|32| | |4.91|25.8|\n| | | |32| | |16|16| | |5.01|25.4|\n|(A)| | | | | |16| | |5.16|25.1|58|\n| | | | | | |32| | |5.01|25.4|60|\n|2| | | | | | |6.11|23.7|36| | |\n|4| | | | | | |5.19|25.3|50| | |\n|8| | | | | | |4.88|25.5|80| | |\n|(C)|256| | | | |32|32|5.75|24.5|28| |\n| |1024| | | | |128|128|4.66|26.0|168| |\n| | |1024| | | | |5.12|25.4|53| | |\n| | |4096| | | | |4.75|26.2|90| | |\n|(D)| | | | | | |5.77|24.6| | | |\n| | | | | | | |4.95|25.5| | | |\n| | | | | | | |4.67|25.3| | | |\n|(E)| | | | | | |4.92|25.7| | | |\n|big|6|1024|4096|16| | |0.3|300K|4.33|26.4|213|\n\nDevelopment set, newstest2013.",
                            "textTemplate": "",
                            "metadataSeparator": "\n",
                            "type": "TEXT",
                            "id": "55b4e300-48bd-4c5b-891f-32dbdaa847eb",
                            "score": 0.3935406015565825
                        },
                        {
                            "id_": "7e5556e8-4d8c-47d9-a192-264fe1e4cf44",
                            "metadata": {
                                "file_path": "./data/attention.pdf"
                            },
                            "excludedEmbedMetadataKeys": [],
                            "excludedLlmMetadataKeys": [],
                            "relationships": {
                                "SOURCE": {
                                    "nodeId": "5115a8e1-da8f-40cb-80fd-5a6c07fa4443",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "Xe9oUa6hljMJA4FitZvOJsOM9IRi6HlCYSnps2A8qa8="
                                },
                                "PREVIOUS": {
                                    "nodeId": "41f134b6-5549-4a55-adda-379841b7a95a",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "T4odiZzspFlncORI3rhYTLTTLKpd6LpMTBJa3rAL1ik="
                                },
                                "NEXT": {
                                    "nodeId": "d22caf10-4c95-487f-a742-590c71f16fbe",
                                    "metadata": {
                                        "file_path": "./data/attention.pdf"
                                    },
                                    "hash": "fMDhB5lFpOkY5qF5ZMXCDxCtuZ01/BnPikHlXhfLrws="
                                }
                            },
                            "hash": "dkRuRHODoS6tE8sRY0ZqGc6lNBMwAuKAi5zwr943gMY=",
                            "text": "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. ### Position-wise Feed-Forward Networks\n\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 (2)\n\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality d = 2048. ### Embeddings and Softmax\n\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel. ---\n## Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types\n\n|Layer Type|Complexity per Layer|Sequential Operations|Maximum Path Length|\n|---|---|---|---|\n|Self-Attention|O(n^2 · d)|O(1)|O(1)|\n|Recurrent|O(n · d^2)|O(n)|O(n)|\n|Convolutional|O(k · n · d^2)|O(1)|O(log k (n))|\n|Self-Attention (restricted)|O(r · n · d)|O(1)|O(n/r)|\n\n## Positional Encoding\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.",
                            "textTemplate": "",
                            "metadataSeparator": "\n",
                            "type": "TEXT",
                            "id": "7e5556e8-4d8c-47d9-a192-264fe1e4cf44",
                            "score": 0.38383020140684193
                        }
                    ]
                }
            }
        ]
    }
]